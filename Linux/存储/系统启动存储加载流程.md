# Linux 系统启动存储加载流程

## 核心流程概览

为了方便理解，我们将流程划分为四个阶段：

1. **硬件发现与根文件系统准备** (Kernel & Initramfs)
2. **基础文件系统挂载** (Systemd & Fstab)
3. **高级存储与虚拟磁盘映射** (Loop/NBD & Custom Units)
4. **容器存储层初始化** (Docker Daemon & OverlayFS)



### 1. 物理磁盘识别与 RootFS

这是启动的最早期，目标是让内核“看到”硬盘并挂载根目录。

1. **硬件探测 (Kernel Space)**
   - **加载驱动**：Bootloader (GRUB) 加载内核 (`vmlinuz`)。内核启动后，首先加载存储控制器驱动（如 `ahci` 对应 SATA，`nvme` 对应 NVMe，`mpt3sas` 对应 SAS 卡）。
   - **设备注册**：内核扫描总线，识别物理磁盘。此时，内核会在 `/sys` 下创建设备对象，并由 **udev**（此时通常由 initramfs 中的 udevd 处理）在 `/dev` 下创建设备节点，例如 `/dev/sda` 或 `/dev/nvme0n1`。
2. **Initramfs/Initrd (临时文件系统)**
   - 如果根文件系统 (`/`) 位于 LVM、软件 RAID (mdadm) 或加密分区 (LUKS) 上，内核自己是无法直接挂载的。
   - **Initramfs 脚本执行**：
     - 加载必要的内核模块（如 `dm-mod`, `ext4`）。
     - **LVM/RAID 组装**：运行 `pvscan`, `vgchange -ay` 激活卷组，将物理块设备映射为逻辑块设备（如 `/dev/mapper/rl-root`）。
   - **挂载 Root**：Initramfs 将真正的根分区以**只读 (Read-Only)** 模式挂载到临时目录，然后通过 `switch_root` 将控制权交给真正的系统。



### 2. 基础文件系统挂载

Systemd (PID 1) 接管系统后，开始根据配置挂载文件系统。

1. **解析 `/etc/fstab` (systemd-fstab-generator)**
   - Systemd 并不直接读取 `/etc/fstab` 执行挂载，而是使用 **Generators** 机制。
   - **机制**：在启动早期，`systemd-fstab-generator` 会读取 `/etc/fstab`，并为每一行记录动态生成一个 Systemd Unit 文件（`.mount` 类型）。
   - **产物**：例如 `/home` 分区会被转换为 `home.mount` 单元，存放在 `/run/systemd/generator/` 中。
2. **根目录重挂载 (Remount Root)**
   - 系统启动初期根目录是只读的。Systemd 会执行 `systemd-remount-fs.service`，根据 fstab 的配置（通常是 `defaults` 或 `rw`），将根目录重新挂载为**读写 (Read-Write)** 模式。
3. **本地文件系统挂载 (Local FS)**
   - Systemd 激活 `local-fs.target`。
   - 所有依赖于此 target 的 `.mount` 单元并行启动。此时，物理磁盘上的分区（如 `/boot`, `/home`, `/var`）被挂载到目录树中。



### 3. 虚拟磁盘映射

**注意**：这种操作通常发生在 `local-fs.target` 之后，因为必须先挂载物理磁盘（存有 qcow2 文件的磁盘），才能读取该文件进行映射。



这是你提到的重点：如何处理存储在物理文件系统中的虚拟磁盘文件（如 `.img`, `.iso`, `.qcow2`, `.vmdk`）。

这部分通常**不是自动的**，除非配置了特定的 Systemd 服务或 fstab 条目。



**简单的 Raw 镜像 (`.img` / `.iso`)**

- **机制**：使用 Linux 的 **Loop 设备** (`/dev/loopX`)。
- **启动流程**：
  - 如果在 `/etc/fstab` 中配置了挂载 ISO 或 img 文件，`systemd-fstab-generator` 会自动处理。
  - 它会调用 `losetup` 命令，将文件（如 `/data/images/disk.img`）映射为一个块设备（如 `/dev/loop0`）。
  - 然后执行 `mount /dev/loop0 /mnt/point`。



**复杂的虚拟格式 (`.qcow2` / `.vmdk`)**

Linux 内核不能直接挂载 qcow2（那是 QEMU 的格式）。要在宿主机启动时将其映射为块设备，通常需要用户空间的工具（如 `qemu-nbd` - Network Block Device）。

- **场景**：你想在宿主机直接读写虚拟机磁盘。
- **Systemd 流程 (需自定义)**：
  1. **加载模块**：内核加载 `nbd` 模块 (`modprobe nbd`)。
  2. **映射服务**：创建一个自定义的 Systemd Service（例如 `map-vmdk.service`）。
     - `ExecStart=/usr/bin/qemu-nbd --connect=/dev/nbd0 /data/vm/disk.qcow2`
  3. **挂载**：
     - 一旦 `/dev/nbd0` 被创建，Systemd (或 mount 命令) 就可以像对待物理硬盘一样对待它。
     - 如果磁盘内有分区（如 `/dev/nbd0p1`），则挂载该分区。



**Udev 触发与 LVM 过滤器**

当你在系统启动中后期（Post-boot）通过 `losetup` 或 `qemu-nbd` 映射一个设备时，内核会触发一个 **Udev Event (ADD)**。

理论上，LVM 的 Udev 规则（通常位于 `/usr/lib/udev/rules.d/69-dm-lvm-metad.rules` 或类似位置）会捕获这个事件并运行 `pvscan`。但是，这一步通常会被**拦截**：



`lvm.conf` 中的过滤器 (Filters)

这是最主要的拦截层。打开 `/etc/lvm/lvm.conf`，你会看到 `filter` 或 `global_filter` 配置。

- **Loop 设备**：大多数发行版默认配置会**忽略** `/dev/loopX` 设备。
  - 原因：Loop 设备通常不稳定，且可能导致内存死锁（因为 Loop 本身依赖底层文件系统，底层文件系统又可能依赖内存分配，如果 LVM 卷也在上面，容易形成闭环）。
- **NBD (Network Block Device)**：通常也不在白名单中，或者因为网络未就绪而被忽略。



### 4. Docker 卷与容器存储

当物理存储和虚拟映射都就绪后，Docker 守护进程启动，开始构建容器的存储层。

1. **Docker Daemon 启动 (`dockerd`)**
   - Systemd 启动 `docker.service` (通常在 `multi-user.target` 阶段)。
   - Docker 检查其存储根目录（通常是 `/var/lib/docker`）。
2. **存储驱动初始化 (Storage Driver)**
   - Docker 会根据配置（默认通常是 **Overlay2**）初始化存储驱动。
   - **OverlayFS 原理**：它利用内核的 OverlayFS 特性，将底层的只读镜像层（LowerDir）和上层的读写层（UpperDir）合并挂载为一个统一的视图（MergedDir）。
   - **Backing Filesystem**：Overlay2 依赖于底层的物理文件系统（通常要求是 xfs 或 ext4，且支持 d_type）。
3. **Docker Volume (卷) 的挂载** 当一个容器启动时，Docker 处理两种类型的存储挂载：
   - **Bind Mounts (绑定挂载)**：
     - 直接利用 Linux 的 `mount --bind` 功能。
     - 命令：`mount --bind /host/path /var/lib/docker/overlay2/.../merged/container/path`。
     - 这使得物理磁盘上的某个目录直接暴露给容器内部。
   - **Docker Managed Volumes**：
     - Docker 在 `/var/lib/docker/volumes/` 下创建一个目录。
     - 然后同样使用 Bind Mount 将其挂载到容器内。
     - 如果是第三方存储插件（如 NFS, Ceph, Portworx），Docker 会调用插件驱动，先在宿主机上完成网络存储的挂载，然后再 Bind Mount 到容器中。